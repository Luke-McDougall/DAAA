* Algorithm 
  - An algorithm is any well-defined computational procedure that
    - takes some value as input
    - produces some value as output
    - solves a specified computational problem
  - An algorithm
    - must be correct (i.e., always gives the right result)
    - can be specified in English, as a computer program, or as
      hardware design.
* Problem Example 1
  - Sorting problem :: A problem you have learnt before...
  - Input :: a sequence of /n/ numbers (a1, a2, ..., an)
  - Output :: A reordering (b1, b2, ..., bn) of input sequences such
              that b1 <= b2 <= ... <= bn.
  - Example
  - Input :: (30, 20, 41, 51, 3, 20)
  - Output :: (3, 20, 20, 30, 41, 51)


  =INSERTION-SORT (A)=
  #+BEGIN_SRC c
    for(int j = 1; j < A.length; j++)
    {
        int key = A[j];
        // insert A[j] into the sorted sequence A[1 ... j-1]
        int i = j - 1;
        while(i > 0 && A[i] > key)
        {
            A[i + 1] = A[i];
            i--;
        }
        A[i + 1] = key;
    }
  #+END_SRC
  - A.length is the total number of elements in array A.
  - Insertion sort (IS) is /in-place/ sorting algorithm: the numbers
    are rearranged within the array A; it requires only a constant
    amount of memory for temporary variables, loop control variables,
    sentinel, etc.
  - IS is a /stable/ sorting algorithm: it maintains a relative order
    of repeated elements.
  - IS is fast for small sized input and for input that is almost
    sorted.
* Problem Example 2
  - Greatest Common Divisor (GCD) :: learnt in high school...
  - Input :: Integers X and Y
  - Output :: The largest integer Z that divides both X and Y, i.e., Z
              = GCD(X, Y)
  - Note :: GCD(X, Y) = GCD(Y, X)
  - Example:
  - Input :: X = 1035, Y = 759
  - Output :: Z = GCD(1035, 759) = 69
** Algorithm 1
   - Step 1 :: Find all *prime factors* of both X and Y
   - Step 2 :: Multiply all *common* prime factors to form Z
   - Example:
   - Step 1 :: 1035 = 3^2 * 5 * 23 and 759 = 3 * 11 * 23
     - The common prime factors are: 3 and 23
   - Step 2 :: Z = 3 * 23 = 69
** Algorithm 2
   For two positive integers X and Y, where X >= Y, we have GCD(X, Y)
   = GCD(X *mod* Y, Y)
   - Note :: You can use a *recursive* or *iterative* function to
             implement algorithm 2
   - Recursive version
     #+BEGIN_SRC c
       int gcd(int x, int y)
       {
           if(y == 0) return x;
           return gcd(y, x % y);
       }
     #+END_SRC
   - Example :: X = 1035, Y = 759
     1. GCD(1035, 759) = GCD(1035 mod 759, 759) = GCD(276, 759)
     2. GCD(759, 276) = GCD(759 mod 276, 276) = GCD(207, 276)
     3. GCD(276, 207) = GCD(276 mod 207, 207) = GCD(69, 207)
     4. GCD(207, 69) = GCD(207 mod 69, 69) = GCD(0, 69) = GCD(69, 0)
* Problem Example 3
  - Least Common Multiple (LCM) :: also from high school...
  - Input :: Integers *X* and *Y*
  - Output :: The smallest integer *Z* divisible by both *X* and *Y*,
              i.e., Z = LCM(X, Y)
  - Note :: LCM(C, Y) = LCM(Y, X)
  - Example
  - Input :: X = 1035, Y = 759
  - Output :: Z = LCM(1035, 759) = 11385
** Algorithm 1
   - Step 1 :: Find all *prime factors* of both X and Y
   - Step 2 :: Multiply all prime factors to form Z
     - For each prime factor common to X and Y, use the largest power.
   - Example :: Find the LCM of X = 1035 and Y = 759
   - Step 1 :: 1035 = 3^2 * 5 * 23 and 759 = 3 * 11 * 23
     - The common prime factors are: 3 and 23
   - Step 2 :: Z = 3^2 * 5 * 11 * 23 = 11385
** Algorithm 2
   We can use the solution of GCD(X, Y) to compute LCM(X, Y) as
   follows. LCM(X, Y) = (X * Y) / GCD(X, Y). 
   - Example :: X = 1035, Y = 759
     - LCM(1035, 759) = (1035 * 759) / GCD(1035, 759)
     - From previous example, we have GCD(1035, 759) = 69
     - (1035 * 759) / 69 = 11385
* Algorithm as Technology
  - Algorithms devised for the same problem often differ dramatically
    in their efficiency
    - Which algorithm for each example is the most efficient?
  - Example for sorting problem:
    - Insertion (IS) sort takes c1 * n^2 to sort /n/ items.
      - Analysis of time complexity discussed later.
    - Merge sort (MS) takes time c2 * n * log(n)
    - Assume /constants/ c1 = 2, c2 = 50
    - Assume you use the algorithms to sort n = 10^6 elements.
    - Assume CPU *A* runs *IS*, and CPU *B* runs *MS*
      - CPU *A* executes 10^9 instructions/sec while CPU *B* executes
        10^7 instructions/sec
        - Thus CPU B is 100 times slower as compared to CPU A
    - *A* takes: 2*(10^6)^2 ~instructions~
                 ------------------------- = 2000 seconds
                 10^9 ~instructions/sec~

    - *B* takes: 50*10^6*log(10^6) ~instructions~
                 ---------------------------------- = 100 seconds
                 10^7 ~instructions/sec~

    - *MS* runs faster even using a slower CPU and assuming larger
      constant value!

      - This example shows the importance of designing more efficient
        algorithms for faster problem solutions!
* How to design an algorithm?
  - Reduce the problem to one with good known solution
  - Write a brute force algorithm and use "tricks" to improve it
  - This unit (DAAA) focuses on time-efficient algorithm
  - Another unit, Theoretical Foundations of Computer Science, covers
    problems
    - with no known efficient solution :: NP-complete
    - with no solution :: undecidable
** Trick 1
   - Use better data structures
     - Often the implementation of an algorithm can be improved by
       choosing appropriate data structures
     - List vs. Arrays for split/join
     - Heaps for repeated min/max
     - Balanced trees or hashing for fast search
     - Graphs, etc
** Trick 2
   - Preprocessing the input (e.g. sorting)
     - Allows binary search vs. linear search
     - Bring important items to the front of a list
     - Attempt to format the data so the algorithm performs at its
       best, rather than at its worst.
** Trick 3
   - Use good algorithm design techniques
     - =Divide & Conquer technique=
       - Split a problem into sub parts
       - Solve each part
       - Re-join to get a solution
     - =Greedy approach=
       - Define a "cost"
         - Cost can be monetary value, time, etc.
       - Sort items by best impact on cost
       - Greedily choose the best until problem solved
       - Note that the solution/result may or may not be "optimal"
     - =Dynamic programming=
* Algorithm Analysis
  - Efficiency measure:
    - Speed :: How long an algorithm takes to produce results
      - This is the usual measure
    - Space/memory :: How much memory is required to run the
                      algorithm.
      - This measure is less commonly used
  - In general, the time taken by an algorithm grows with the size of
    input
    - Input size :: Depends on problems being studied (e.g. number of
                    elements, nodes, links, etc.)
    - What is the input size of a sorting algorithm?
  - Use the same *computational model* for analysed algorithms
    - Running time on particular input: number of primitive operations
      or steps executed using the computation model/computer
* Computation Model
  - The usual (often not stated) computation model is the Random
    Access Machine (RAM)
    - Sequential - RAM executes one instruction at a time
    - RAM contains instructions of real computers: arithmetic
      (e.g. *add*, *multiply*), data movement (e.g. *load*, *store*),
      control (e.g. *if*, *subroutine*, *call*).
    - Each instruction takes a constant amount of time/step
    - Running time: the total number of steps
  - Data type: integer and floating point
  - We need to limit the size of each word of data used
** Example
   | Instruction | Steps | Instruction | Steps |
   |-------------+-------+-------------+-------|
   | n = 1       |     1 | n = 2       |     1 |
   | n = n + 1   |     2 | print n     |     1 |
   | print n     |     1 | ---         |   --- |
   | Total       |     4 | Total       |     2 |

   - Assignment operation is 1 step in RAM model
   - Addition operation is 1 step in RAM model
   - Let's say print operation is 1 step in RAM model
* Analysis of Insertion Sort
  #+BEGIN_SRC c
    for(int j = 1; j < A.length; j++)
    {
        int key = A[j];
        int i = j - 1;

        while(i > 0 && A[i] > key)
        {
            A[i + 1] = A[i];
            i = i - 1;
        }
        A[i + 1] = key;
    }
  #+END_SRC
  | Insertion Sort(A)                 | cost | times               |
  |-----------------------------------+------+---------------------|
  | for(int j = 1; j < A.length; j++) | c1   | n                   |
  | int key = A[j];                   | c2   | n - 1               |
  | int i = j - 1                     | c4   | n - 1               |
  | while(i > 0 && A[i] > key)        | c5   | sum:j=1:n: (tⱼ + 1) |
  | A[i + 1] = A[i];                  | c6   | sum:j=1:n: (tⱼ)     |
  | i = i - 1                         | c7   | sum:j=1:n: (tⱼ)     |
  | A[i + 1] = key                    | c8   | n - 1               |
  
  ci = the constant number of steps used to execute the operation in
  line i
  tⱼ = the number of elements the jth key has to travel to get in it's
  proper place.
  
  T(n) = c1*n + c2*(n-1) + c4*(n-1) + c5*sum:j=1:n:(tⱼ + 1) +
  c6*sum:j=1:n:(tⱼ) + c7*sum:j=1:n:(tⱼ) + c8*(n-1)

  =So, running time, T(n), depends on input size n = A.length=

  - Best Case :: When the array is already sorted
    - Line 5: IF A[i] <= key then tⱼ = 0 for all j.
  - Thus,
    c5*sum:j=1:n:(tⱼ + 1) = c5*sum:j=1:n:(0 + 1) = c5*(n - 1)
    
    c6*sum:j=1:n:(tⱼ) = c7*sum:j=1:n:(tⱼ) = 0

    T(n) c1*n + c2*(n-1) + c4*(n-1) + c5*(n-1) + 0 + 0 + c8*(n-1)
    =    (c1 + c2 + c4 + c5 + c8)*n - (c2 + c4 + c5 + c8)
    =    a*n + b
  - Worst Case :: the *longest running time* for an input of size n
    - the array is in reverse sorted order
    - Must compare each A[j] with each element in sorted sub array
      A[1...j - 1]
      - tⱼ = j, for j = 1, 2,...,n
      Thus,                                          n*(n + 1)
      c5*sum:j=1:n:(tⱼ + 1) = c5*sum:j=1:n:(j + 1) = --------- - 1
                                                        2
                                            (n-1)*n
     c6*sum:j=1:n:(tⱼ) = c6*sum:j=1:n:(j) = ------- and c7 as well. 
                                               2
     
     T(n) = c1*n + c2*(n-1) + c4*(n-1) + c5*(n*(n + 1)/2 -1) +
      c6*((n-1)*n/2) + c7*((n-1)*n/2) + c8*(n-1)
     = (c1+c2+c4+c8)*n - (c2+c4+c8) + (c6 +
      c7)*((n-1)*n/2) + c5*(n*(n+1)/2 -1)
      
=working out time=
     n^2 + n                    n^2 - n
c5* (------- - 1),  (c6 + c7)* (-------)
        2                          2
        
n^2*c5 + n*c5         c6*n^2 + c7*n^2 - c6*n - c7*n
------------- - c5 +  -----------------------------
      2                           2
      
(c5 + c6 + c7)*n^2 + (c5 - c6 - c7)*n
-------------------------------------  - c5
                 2
                 
 c5   c6   c7                       c5   c6   c7
(-- + -- + --)* n^2 + (c1+c2+c4+c8+ -- - -- - --)*n - (c2+c4+c8-c5)
 2    2    2                        2    2    2
 
a*n^2 + b*n + d
=end of working out time=
~How do you like my ascii math formulas?~
* Growth of functions
  - Consider an^2 + bn + c
    - For large /n/, the value of /bn + c/ is relatively insignificant
      to the value of an^2
  - Consider an^2
    - For large /n/, the constant coefficient /a/ is less significant
      than the rate of growth of n
  - Thus we express an^2 + bn + c as O(n^2)
    - Read as "Big theta of n-squared" (pretend it's a theta symbol
      not just the letter "o" pls).
* O(n) - Big-oh of n
  - *Asymptotic /upper/ bound*
    - A given function /f(n)/ is /O(g(n))/ if there exist positive
      constants c and n₀ such that: ~0≤f(n)≤c*g(n)~ for all n ≥ n₀.
    - O(g(n)) represents a set of functions 
      {f(n):∃c>0,n₀>0 such that 0≤f(n)≤c·g(n),∀n≥n₀}
      ∃ is read as: there exists, there is, or there are.
      ∀ is read as: for all, for any, or for each
** O example 
   Show that 2n + 6 = O(n)
   - 0≤2n + 6 ≤ cn, definition of Big O
   - 0≤2 + 6/n≤c,   divide by n≥n₀
   - TRUE for n≥1 and c≥8
   - TRUE for n≥2 and c≥5

   You can find other possible constants, but you need to show only
   one pair of all possible constants
** Back to the example
*** Big O calculation
   sum = 0                  =1 assignment=
   for i = 1 to n           =n+1 assignments and n+1 comparisons=
           sum = sum + A[i] =n assignment, 2n additions, n memory=
                                                         =access=

   - What is the order (big-O) of the first line?
     - It's a constant, so any constant would do as big-O
       - The convention is to use /O(1)/
   - This says that there is some constant c such that
     - 0 ≤ 1 ≤ c × 1 for all sufficient high value of n.
   - It can be seen that any c ≥ 1 would do.
   - What is the order of the second line?
     - It is executed (n + 1) times and contains one comparison and
       one assignment for a total complexity of 2n + 2.
   - What is a suitable upper bound for a sufficiently large /n/?
     - 3n, 4n, 5.732n, 2πn, and other possible values of c and n
     - This makes it O(n)
   - Four steps repeated n times is O(n).
   - Hence the fragment is O(1) + O(n) + O(n) for a total of O(n)
     - Because we can add individual constants, just take the largest
       term when adding.
*** Alternative calculation 
    | Thing            | cost | times |
    |------------------+------+-------|
    | sum = 0          | c₁   | 1     |
    | for i = 1 to n   | c₂   | n+1   |
    | sum = sum + A[i] | c₃   | n     |
    T(n) = c₁×1 + c₂×(n+1) + c₃×n
    = (c₂ + c₃)×n + (c₁ + c₂)
    = a×n + b
    Thus Big-O of O(n)

    Proof: b + a×n ≤ c×n → TRUE for n ≥ 1 and c ≥ b + a
* Simple Big-O 
  - =All O(1)=
    - s ← 0
    - s + 1
    - s × 1
    - s ≤ 1
  - =O(n)=
    - for i = 0 to n
          some O(1) process
    - sum:i=0:n:(O(1)) = O(n)
  - =Why?=
    - If f(n) is O(1) then ∃c₁,n₀ such that f(n)≤c₁ ∀n>n₀
    - Let the ith term in sum:i=0:n:(O(1)) be bounded by cᵢ,∀n>nᵢ
    - sum:i=o:n:(O(1)) ≤ c₁ + c₂ + ...+ cₙ, ∀n > max(nᵢ)
    - sum:i=0:n:(O(1)) ≤ n × max(c₁,c₂,...,cₙ)
    - sum:i=0:n:(O(1)) is O(n)
  - =O(n²)=
    - for i = 0 to n
          for j = 0 to n
              some O(1) process
    - sum:i=0:n:(sum:j=0:n:(O(1))) is O(n²)
    - for i = 0 to n
          for j = 0 to i
              some O(1) process
    - O(n²), O(f(n)) < O(n²)
* Aim for the least upper bound
  - Many algorithms are O(2ⁿ), but saying an algorithms is O(2ⁿ) is
    not always very useful if that isn't truly representative.
    - An O(n) algorithm is O(2ⁿ); but an O(2ⁿ) algorithm is NOT
      necessarily O(n).
  - Aim for the *smallest* O( g(n)) expression
  - If f(n) is the smallest possible, then it is said to be a *tight*
    *upper bound*;
    - if f(n) is O(n²), it is also O(n³), however, the first
      expression is better
  - *Note: tight /upper/ bound* is different from *asymptotic /tight/
    bound*
    - *asymptotic tight bound* is called *big theta* (θ)
* Ω(n) - big-omega of n
  *Asymptotic /lower/ bound*
  - A given function f(n) is Ω(g(n)) if there exist positive constants
    c and n₀ such that 0 ≤ c·g(n) ≤ f(n) for all n ≥ n₀
  - Ω(g(n)) represents a set of functions:
    - {f(n):∃ c > 0, n₀ > 0 and 0 ≤ c·g(n) ≤ f(n), ∀ n ≥ n₀}
** Ω example
   Show that 2n + 6 = Ω(n)
   1. 0 ≤ c·n ≤ 2n + 6, =definition of big Ω=
   2. 0 ≤ c ≤ 2 + 6/n,  =divide by n≥n₀=
   3. TRUE for c ≤ 2, n ≥ 1
* θ(n) - Big-theta of n
  *Asymptotic /tight/ bound*
  - A given function f(n) is θ(g(n)) if there exists positive
    constants c₀ and c₁ and n₀ such that:
    - 0 ≤ c₀·g(n) ≤ f(n) ≤ c₁g(n) for all n≥n₀
  - θ(g(n)) represents a set of functions:
    - O(g(n)) ∩ Ω(g(n))
  - θ(g(n)) ⊆ O(g(n))
  - θ(g(n)) ⊆ Ω(g(n))
** Example
   Show that ½n²-3n = θ(n²)
   - 0 ≤ c₀·n² ≤ ½n²-3n ≤ c₁n² =definition of big θ=
   - 0 ≤ c₀ ≤ ½ - 3/n ≤ c₁     =divide by n²=
   - 0 ≤ c₀ ≤ ½ - 3/n → TRUE for c₀ ≤ 1/14, n ≥ 7
   - 0 ≤ ½ - 3/n ≤ c₁ → TRUE for c₁ ≥ ½, n ≥ 6
   - ∵ 0 ≤ c₀·n² ≤ ½n²-3n ≤ c₁n² → TRUE for c₀ ≥ 1/14, c₁ ≥ ½, n ≥ 7
** Alternate proof
   Show that ½n²-3n = θ(n²)
   - Prove for Big O and Big Ω separately.
   - Use the larger n₀ between that for Big O and Big Ω
   - *Big O*
     - 0 ≤ ½n²-3n ≤ c₁·n² =definition of big O=
     - 0 ≤ ½ - 3/n ≤ c₁   =divide by n^2=
     - n ≥ 6 so that f(n) is not negative
     - ∴ 0 ≤ ½ - 3/n ≤ c₁ → TRUE for n ≥ 6, c₁ ≥ ½
   - *Big Ω*
     - 0 ≤ c₂·n² ≤ ½n²-3n =definition of big Ω=
     - 0 ≤ c₂ ≤ ½ - 3/n   =divide by n²=
     - n ≥ 7 so that c₂ > 0
     - ∴ 0 ≤ c₂ ≤ ½ - 3/n → TRUE for n ≥7, c₂ ≤ 1/14
   - Select the larger of the n₀ for O and Ω i.e. max(6, 7)
     - ½n²-3n = θ(n²) for c₁ ≥ ½, c₂ ≤ 1/14, n₀ ≥ 7
* Other examples
  #+BEGIN_SRC c
    for(int i = 0; i < n; i++)
    {
        for(int j = 0; j < n; j++)
        {
            // Some O(1) process
        }
    }
  #+END_SRC
  - O(n²)
  - Ω(n²)
  - θ(n²)
  #+BEGIN_SRC c
    for(int i = 0; i < n; i++)
    {
        for(int j = 0; j < i; j++)
        {
            // Some O(1) process
        }
    }
  #+END_SRC
  - O(n²)
  - Ω(n)
** Consider the following algorithm
    #+BEGIN_SRC c
      for(int i = 0; i < n; i++)    // O(n)
      {
          if(A[i] == 0)             // O(1)
          {
              break;                // O(1)
          }
          else                      // O(1)
          {
              sum(A);               // O(n)
          }
      }
    #+END_SRC
  - Worst case
    - The n inputs are chosen so that the algorithm runs for as long
      as possible
    - Worst case running time is O(n²), when A[i] ≠ 0 for 0 ≤ i < n
  - Best case
    - The n inputs are chosen so that the algorithm runs for as little
      as possible
    - Best case running time is O(1), when A[0] = 0
  - Average case
    - The n inputs are assumed to be chosen uniformly random
    - e.g., if A were a permutation of [0..n)
      - 1/n times, stop
      - 1/n times, 1 sum
      - 1/n times, 2 sums
      - 1/n times, 3 sums
      - ...
    - Average case running time is O(n²)
** Expected Case
  - What is the usual or normal input?
  - Still of size n
  - Can be different from average case
  - e.g., a compression algorithm tailored to English text would
    "expect" words of average length 5 characters.
* Algorithm time analysis - Summary
  - Choose model of computation
  - Choose case
    - Worst case.
    - Average case.
    - Best Case
  - You are not free to choose size of the problem n
    - Usually interested in large n
  - Count the steps and report asymptotic time using O, Ω, or θ
    - Express the bounds as tight as possible
      - If f(n) is O(n²), it is also O(n³); present it as O(n²)
      - If f(n) is Ω(n²) it is also Ω(n); present it as Ω(n²)
* Space or memory analysis
  - A model of computation should define a O(1) unit of memory
    - a register or word size ω bits
  - In RAM
    - Numbers use O(1) space
    - A[n] uses O(n) space
    - Binary tree of n leaves is O(???)
  - All assumed data units are less than 2^ω
* Selection sort
  - Input :: unsorted array A
  - Output :: sorted array A
    #+BEGIN_SRC c
      for(int i = 0; i < n - 1; i++)
      {
          int small = i;                 // θ(1) c₀              //
          for(int j = i + 1; j < n; j++)                         //
          {                                                      //
              if(A[j] < A[small])        //        //            //
              {                          //        // ∑j=i+1 to n// ∑i=0 to n-1 
                  small = j;             // θ(1) c₁//            //
              }                          //        //            //
          }                                                      //
          int temp = A[small];          //                       //
          A[small] = A[i];              // θ(1) c₂               //
          A[i] = temp;                  //                       //
      }
    #+END_SRC
  - T(n) = ∑i=0 to n-1 (c₀) + ∑i=0 to n-1 (c₂) + ∑i=0 to n-1 (∑j=i+1
    to n (c₁))
  - = O(n) + O(n) + O(n²)
  - ∴ O(n²) 
* Proof by Induction
  - Induction is very similar to recursion
    - Proof by induction is very useful in analysing *recursive*
      algorithms
  - To prove a statement S(n), proof by induction requires proving two
    cases:
    - Base Case :: prove statement S(n) only for a small value of n,
                   e.g., n = 1 or n = 2.
    - Inductive Step :: prove that if the statement is true for a
                        smaller value of n, for any value of k ≥ n
** Example
   - F(n) =
     - 1, if n < 3
     - F(n-1) + F(n-2), otherwise
   - The following is a sequence of Fibonacci number F(n):
     - 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, ...
   - Problem :: Prove that the nth Fibonacci number is O(Φⁿ)
   - Note :: Φ is the root of x² - x - 1 = 0
     - Φ = ½(1 + √5) ≈ 1.618
   - Base Case
     - By the definition of big O: 0 ≤ F(n) ≤ c·Φⁿ
     - Let n = 3, 0 ≤ F(3) ≤ c·Φ³
     - = 0 ≤ 2 ≤ c·4.236 → TRUE: ∀ c ≥ 0.472
   - Inductive Step
     - Assume 0 ≤ F(k) ≤ c·Φ^κ → TRUE for k > 1 and k ≤ n
     - Given the above prove that 0 ≤ F(K + 1) ≤ c·Φ^(k + 1)
       - F(K) ≤ c·Φ^k, add F(K-1)
       - F(K) + F(K-1) ≤ c·Φ^k + F(K-1), F(K-1) ≤ c·Φ^(K-1)
       - F(K) + F(K-1) ≤ c·Φ^k + c·Φ^(K-1)
       - F(K) + F(K-1) ≤ c·(Φ^k + Φ^(K-1))
* How to analyse a recursive function?
  - Use *recurrence* to analyse the time complexity of a recursive
    function.
    - e.g., T(n) = T(n-1) + n, T(n) = 2T(n/2) + n², etc
  - A *recurrence* is a function defined by:
    - One or more base cases
    - Itself with smaller arguments.
** Methods for solving recurrence:
   - Iteration - not recommended; easy to make mistake!
   - Master method if T(n) = aT(n/b) + f(n)
     - T(n) = 2T(n/2) + n → YES
     - T(n) = T(n-1) + n → NO
   - Substitution method (guess and induction)
     - Use this method if the recurrence cannot be solved by master
       method.
     - Use recursion-tree method for the guess in induction
** Selection Sort (recursive) - analysis
   - Input :: Unsorted array A
   - Output :: Sorted array A
     #+BEGIN_SRC python
       # The examples are in python now. Don't ask why cunt
       def selection_sort(A, n, i):
           if i == n:
               return
           small = i

           for idx in range(i + 1, n + 1):
               if A[idx] < A[small]:
                   small = idx

           A[i], A[small] = A[small], A[i]
           selection_sort(A, n, i + 1)
     #+END_SRC
   - Analysis:
     Let T(n) be the time for this algorithm to operate on /n/
     elements. The *recurrence* for time complexity of Selection Sort is.
   - Base case :: T(1) = θ(1)
   - find small and recursive call :: T(n) = T(n - 1) + O(n)
   - How to sole T(n)?
*** Iteration Method
    - T(n) = T(n-1) + O(n), =overall → iteration k = 0=
    - T(n-1) = T(n-2) + (n-1), =iteration k = 2=
      - T(n) = {T(n-2) + (n-1)} + n = T(n-2) + 2n - 1
        - =1 = 1·(1+1)/2=
    - T(n-2) = T(n-3) + (n-2), =iteration k = 2=
      - T(n) = {T(n-3) + (n-2)} + 2n - 1 = T(n-3) + 3n - 3
        - =3 = 2·(2+1)/2=
    - T(n-3) = T(n-4) + (n-3), =iteration 3=
      - {T(n-4) + (n-3)} + 3n - 3 = T(n-4) + 4n - 6
        - =6 = 3·(3+1)/2=
    - T(n-4) = T(n-5) + (n-4), =iteration 4=
      - T(n) = {T(n-5) + (n-4)} + 4n - 6 = T(n-5) + 5n - 10
        - =10 = 4·(4+1)/2=
    - So in general :: =T(n) = T(n - (k + 1)) + (k + 1)n - k(k + 1)/2=
                       where k = iteration number
      - =T(n) = T(n - (k+1)) + (k+1)(n - k/2)=
      - =T(n) = T(n - (k+1)) + (k+1)(2n - k)/2=
    - When k = n - 2, we have
      - =T(n) = T(n - (n - 2 + 1)) + (n - 2 + 1)(2n - (n - 2))/2=
      - =T(n) = T(1) + (n - 1)(n + 2)/2=
      - =T(n) = T(1) + (n² + n - 2)/2=
      - == O(1) + O(n²) = O(n²)=
*** Guess and Induction
    - Guess the solution for T(n) is O(f(n))
    - Prove T(n) is O(f(n))
    - Assumes answer for small n
      - e.g., n = 2 or n = 1
    - Show it holds for large n
    - Show it holds for base case.
      - Can use n₀ to exclude nasty cases (e.g., when n = 1 → log₂1 = 0)
**** How to Guess? 
     - Solve T(n) = 2T(n/2) + n
     - Use recursion tree to guess solution for T(n)
     - The fully expanded tree has height of log₂n
     - Each level takes n time
     - Thus, the total time is n·log₂n 
**** How to prove by induction
     - The guess for the solution of T(n) is )(n·log₂n)
     - Use induction to show the solution of T(n) = 2T(n/2) + n is O(n·log₂n)
       - Assume T(x) ≤ c·x·log₂x holds when x = n/2
       - T(n) ≤ 2c(n/2)·log(n/2) + n
       - ≤ c·n·log(n/2) + n
       - c·n·log₂n - c·n·log₂2 + n
       - c·n·log₂n - n(c-1)
       - c·n·log₂n if c ≥ 1
     - Now for the base case when n = 1
       - T(1) = 2T(0) + 1  = 1 ≤ c·1·log(1) = 0 → 1 ≤ 0 ???
       - Assume n₀ > 1, so base case is n = 2
       - T(2) = 2T(1) + 2 ≤ c·2·log(2)
       -  = 4 ≤ c·2 → TRUE ∀ c ≥ 2
*** Master Method
    - if T(n) = a·T(n/b) + f(n) then use the following master theorem.
      - θ(n^log_b(a))     f(n) = O(n^((log_b(a) - ε))); means f(n) <
        n^log_b(a) =Case 1=
      - θ(n^(log_b(a))lg(n))  f(n) = θ(n^log_b(a)); means f(n) =
        n^log_b(a) =Case 2=
      - θ(f(n))   f(n) = Ω(n^(log_b(a) + ε)); means f(n) >
        n^(log_b(a)) if a·f·(n/b) ≤ c·f·(n) ∀ c<1 and large =Case3=
      - *Case 1* must meet /polynomial/ condition
        - f(n) must be asymptotically *smaller* than n^log_b(a) by a
          factor of n^ε, for some constant ε > 0.
        - *Case 3* must meet /polynomial/ and /regularity/ conditions
          - f(n) must be asymptotically *larger* than n^log_b(a) by a
            factor of n^ε, for some constant ε > 0.
          - Regularity condition: a·f(n/b) ≤ c·f(n)
            - This condition can be ignored
*** Master Theorem Example
    Give asymptotic tight bound for T(n) = 9T(n/3) + n
    - *Solution*:
    - a = 9, b = 3, and f(n) = n.
    - n^(log_b(a)) = n^(log₃9) = n² → f(n) < n^(log_b(a)) =→ Case 1=
    - *Does it satisfy the polynomial condition?*
    - f(n) = O(n^(log₃9 - ε)) for ε = 1 → YES
    - ∴ T(n) = θ(n²)
    - *Alternatively ...*
    - Compute the ratio between n^(log_b(a)) and f(n)
    - If the ratio is O(n^ε), it satisfies the polynomial condition
    - e.g., n²/n = n¹ → n² is asymptotically larger than n by a factor
      of n^ε for ε = 1
*** Master Theorem Example 2
    Give asymptotic tight bound for T(n) = T(2n/3) + 1
    - *Solution:*
    - a = 1, b = 3/2, and f(n) = 1.
    - n^(log_b(a)) = n^(log_3/2(1)) = n⁰ = 1 → f(n) = n^(log_b(a)) =→
      Case 2=
    - or f(n) = θ(n^(log_b(a)))
    - ∴ T(n) = θ(log(n))
*** Master Theorem Example 3.1
    Give asymptotic tight bound for T(n) = 3T(n/4) + n·log(n)
    - *Solution:*
    - a = 3, b = 4, and f(n) = n·log(n)
    - n^(log_b(a)) = n^(log₄3) = n^(0.793) → f(n) > n^(log_b(a)) =→
      Case 3=
    - *Does it satisfy the polynomial condition?*
    - f(n) = Ω(n^(log₄3 + ε)), for ε ≈ 0.2 or f(n) > log₄3 → YES
    - *Alternatively ...*
    - f(n)/n^(log_b(a)) = n·lg(n)/n^(log₄3) = n·lg(n)/n^0.793 ≥ n^ε
      for ε ≈ 0.2
    - *Note: n·lg(n) ≥ c·n^(log₄3)·n^0.2*
    - *Does it satisfy the regularity condition?* a·f(n/b) ≤ c·f(n)
    - a = 3, b = 4, and f(n) = n·log(n)
    - a·f(n/b) = 3(n/4)·log(n/4)
    - ≤ (3/4)·n·log(n)
    - ≤ c·f(n), for c ≥ 3/4 → YES
    - Following case 3: T(n) = θ(n·lg(n))
*** Master Theorem Example 3.2
    Give asymptotic tight bound for T(n) = 2T(n/2) + n·log(n)
     - *Solution:*
     - a = 2, b = 2, and f(n) = n·log(n)
     - n^(log_b(a)) = n^(log₂2) = n → f(n) > n^(log_b(a)) =→ Case 3=
     - *Does it satisfy the polynomial condition? NO*
     - f(n)/n^(log_b(a)) = (n·log(n))/n = log(n) is asymptotically
       less than n^ε
     - =Thus, we cannot use the master method to solve the recurrence=
* How to prove an algorithm is correct?
  - Use /loop invariant/ to prove its correctness.
  - Three things to look at in loop invariant:
    - *Initialisation*: /It is true prior to the first iteration of
      the loop/
    - *Maintenance*: /If it is true before an iteration of the loop,
      it remains true before the next iteration/
    - *Termination*: /When the loop terminates, the invariant gives a
      useful property that help shows that the algorithm is correct/
  - The proof using loop invariant is similar to the proof by
    /induction/:
    - *Initialisation* is the *base case* in induction
    - *Maintenance* is the *inductive step* in induction
    - *Termination* - there is a condition that makes the loop to
      terminate
      - It is different from that in induction that uses inductive
        step /infinitely/.
* Is the insertion sort correct?
  - *INSERTION-SORT(A)*
    #+BEGIN_SRC c
      for(int j = 1; j < A.length; j++)
      {
          int key = A[j];
          int i = j - 1;

          while(i > 0 && A[i] > key)
          {
              A[i + 1] = A[i];
              i--;
          }
          A[i + 1] = key
      }
    #+END_SRC

  - *Loop invariant:* At the start of the for loop, A contains the
    same elements as its original contents but in sorted order

  - *Initialisation:* When j = 1, subarray A[0 ...j - 1] contains only
    one /sorted/ element A[0]
    - it holds prior to the first loop iteration.
  - *Maintenance:*
    - For each key = A[j], assume it is true that A[0 ...j - 1] is a
      sorted sequence
      - We need to show that after each loop, A[0 ...j] is also sorted.
      - Line 4-7 move each A[j-1] to its next position on right until
        the right position for key is found
        - Thus, after inserting key, A[0 ..j] is a sorted sequence,
          which will be used for the next iteration of the loop, with
          j+1.

  - *Termination*:
    - The loop terminates when j = A.length = n. Each iteration always
      increments the value of j by one.
    - Thus, eventually, the value of j will reach n + 1, and the loop
      will terminate with a sorted A[0 ...n]
  - Hence the insertion sort is correct.
  
  =NOTE: A more complex algorithm may require a more complex / formal
  proof to show its correctness.= 
